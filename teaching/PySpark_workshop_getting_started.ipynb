{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDfYQD8eC6Ui"
   },
   "source": [
    "<h1>UCL School of Management</h1>\n",
    "<h2>MSIN0166 Data Engineering</h2>\n",
    "<h4>PySpark workshop - Theory & Practice</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDY5mUOLX6wU"
   },
   "source": [
    "<img src=\"https://ogirardot.files.wordpress.com/2015/05/future-of-spark.png\"></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmvlOQ6iFDQ1"
   },
   "source": [
    "# What is Spark?\n",
    "\n",
    " Apache Spark™ is a unified analytics engine for large-scale data processing. (Source: https://spark.apache.org)\n",
    "\n",
    "\n",
    "\n",
    "- For more details, go through Spark documentation: https://spark.apache.org/docs/latest/\n",
    "\n",
    "# Why use Spark ?\n",
    "\n",
    "- Speed : Spark framework can be up to 100 times faster than Hadoop when it comes to large scale data processing due to its in memory computing capability. \n",
    "\n",
    "- Ease of use: Spark's popularity led to significant open source community contributions. Databricks, its parent company, implemented a wide range of APIs for streaming solutions, machine learning or graph processing\n",
    "\n",
    "- Fault tolerance is guaranteed without data replication \n",
    "\n",
    "# Current Spark use cases\n",
    "\n",
    "- Netflix uses Spark for providing personalized trailers in real-time\n",
    "- Uber uses Spark for detecting driver abuse at global scale\n",
    "- Spotify uses Spark for creating your \"Wrapped 2019\" review.\n",
    "- Twitter uses Spark for identifying trending twitter hashtags. \n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJd7xhVyFsFU"
   },
   "source": [
    "<h1>Spark architecture</h1>\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\"></img>\n",
    "\n",
    "For a detailed explanation of how Spark runs, please visit: https://spark.apache.org/docs/latest/cluster-overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTPChqnyME8s"
   },
   "source": [
    "## Spark basic concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VctCDmmwOdoX"
   },
   "source": [
    "\n",
    "<img src=\"https://data-flair.training/blogs/wp-content/uploads/sites/2/2017/08/Internals-of-job-execution-in-spark.jpg\"></img>\n",
    "## Key elements\n",
    "- **Driver program**: This element runs the application\n",
    "- **SparkContext**: This is an object providing the application interface\n",
    "- **Cluster Manager**: Manages worker nodes\n",
    "- **Task**: An operation to be executed by the Spark application\n",
    "- **Job**: A collection of tasks\n",
    "- **Resilient Distributed Dataset**: more below\n",
    "- **Directed Acyclic Graph**: A sequence of tasks\n",
    "- **Worker node** : Executes tasks\n",
    "- **Cache**: Data storage used for future faster retrieval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Pk3QLigNl-S"
   },
   "source": [
    "<H3>Apache Spark libraries</h3>\n",
    "\n",
    "- **Spark Streaming**: Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams. (Source: https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"></img>\n",
    "*Spark Streaming receives real-time data from various sources (e.g. Kafka message queues), divides it into batches and sends it to the Spark Engine, which generates the results stream in batches*\n",
    "\n",
    "For more details, read the documentation: https://spark.apache.org/docs/latest/streaming-programming-guide.html\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- **Spark MLLib**: MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "    - ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "    - Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "    - Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "    - Persistence: saving and load algorithms, models, and Pipelines\n",
    "    - Utilities: linear algebra, statistics, data handling, etc.\n",
    "Source: https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "\n",
    "- **Spark GraphX** : GraphX is a new component in Spark for graphs and graph-parallel computation. At a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge. To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/graphx-programming-guide.html\n",
    "\n",
    "- **Spark SQL**: Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine. It also provides powerful integration with the rest of the Spark ecosystem (e.g., integrating SQL query processing with machine learning).\n",
    "\n",
    "\n",
    "**Did you know?** \n",
    "\n",
    "Since its release, Apache Spark, the unified analytics engine, has seen rapid adoption by enterprises across a wide range of industries. Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. It has quickly become the largest open source community in big data, with over 1000 contributors from 250+ organizations.\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsT8v_2RSuWs"
   },
   "source": [
    "## Spark RDDs, DataSets and DataFrames\n",
    "\n",
    "\n",
    "- RDD\n",
    " \n",
    " - As data cannot fit into one single machine, it has to be distributed across multiple nodes for future processing.\n",
    " - Therefore, Spark partitions the data and distributes it across nodes. \n",
    "\n",
    "  - Spark brings the concept of a **resilient distributed dataset (RDD)**: a fault-tolerant collection of elements that can be operated on in parallel. \n",
    "\n",
    "    1. Resilient: An RDD has the ability to recover quickly, yet it comes with an additional requirement: **immutability**. When writing a Spark program, we create a directed acyclical graph consisting of a set of tasks, each generating an RDD after every \"transform\" operation (more on this below). \n",
    "\n",
    "    2. Distributed: The object is distributed across multiple nodes, dealing with fault tolerance.\n",
    "\n",
    "    3. Dataset: Set of data points.\n",
    "\n",
    "  - RDDs contain partitions of data and provide a programming interface\n",
    "\n",
    "  - There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.\n",
    "\n",
    "  - You could think of an RDD as being an immutable, partitioned collection of data points.\n",
    "\n",
    "\n",
    "\n",
    "  - RDD objects manage the interaction with distributeddata transparently\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- **Datasets**: A Dataset is a strongly typed collection of domain-specific objects that can be transformed in parallel using functional or relational operations. Each Dataset also has an untyped view called a DataFrame, which is a Dataset of Row. A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. \n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "- **DataFrames**: A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset<Row> to represent a DataFrame.\n",
    "<br/>\n",
    "\n",
    "Source: https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FthPI_47OGBr"
   },
   "source": [
    "## Spark actions and transformations\n",
    "\n",
    "- Actions:Calculate output values and trigger transformations\n",
    "\n",
    "Read the Spark documentation to see specific examples of Spark actions: https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "- Transformations: Create a new RDD with lazy execution\n",
    "\n",
    "Read the Spark documentation to see specific examples of Spark transformations: https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AycPkkKxOGUL"
   },
   "source": [
    "## Spark caching & storage\n",
    "\n",
    "Spark is popular due to its fast, in-memory capabilities. These capabilities are achieved through persistence (or caching) among other optimizations.  \n",
    "\n",
    "Spark can cache a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). \n",
    "\n",
    "This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative\n",
    " algorithms and fast interactive use.\n",
    "\n",
    "Source: https://spark.apache.org\n",
    "\n",
    "**Spark storage levels**\n",
    "\n",
    "  - MEMORY_ONLY: store in memory, recompute when out of memory \n",
    "    - Typically the best option for repeated reading.\n",
    "  - MEMORY_AND_DISK: store in memory, save to disk when out of memory \n",
    "    - May be slower than recomputingunless computation is extensive\n",
    "  - MEMORY_AND_DISK_2: replicate each partition on 2 cluster node for fast failure recovery  \n",
    "  - DISK_ONLY:  Store the RDD partitions only on disk. \n",
    "\n",
    "\n",
    "**Persistent RDDs**\n",
    "\n",
    "For a better understanding of these concepts, read the documentation: https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#rdd-persistence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyJ6hHT3NmPn"
   },
   "source": [
    "## File systems\n",
    "\n",
    "**What is a file system**\n",
    "\n",
    "\n",
    "As volumes of data grow exponentially throughout the years, the open-source community and the large technology companies came up with data storage solutions in the form of distributed file systems for large, distributed, data-intensive applications.\n",
    "\n",
    "- **HDFS**: Hadoop Distributed File System: \n",
    "\n",
    "The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets.\n",
    "\n",
    "Source: https://hadoop.apache.org\n",
    "\n",
    "Other systems:\n",
    "- **GFS**: Google File System\n",
    "\n",
    "Read more about the Google File System in the original paper: https://research.google/pubs/pub51/\n",
    "\n",
    "\n",
    "- **EFS** : Amazon Elastic File System\n",
    "\n",
    "Read more about Amazon Elastic File System at: https://aws.amazon.com/efs/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6COTmPlLNmgc"
   },
   "source": [
    "## MapReduce \n",
    "\n",
    "**MapReduce algorithm**: A distributed data processing algorithm, inspired by the functional programming paradigm and introduced by Google. \n",
    "\n",
    "MapReduce Algorithm uses the following three main steps:\n",
    "- A **Map** function: This takes a dataset (or a task) and divides it into smaller datasets (or subtasks). It then performs the required computation on each of the data samples (sub-tasks), in parallel. \n",
    "  - It performs the following two steps:\n",
    "    - Splits the input data into smaller sub-datasets\n",
    "    - Performs a mapping step on each sub-dataset.\n",
    "\n",
    "  -The output of this map function is a set of key-value pairs in the following form: <key,value> \n",
    "\n",
    "\n",
    "- A **Shuffle** function: This function takes a list of outputs from the map function and performs two steps:\n",
    "\n",
    "1. Merges all key-value pairs which have the same key, returning a <Key, List<Value>> pair.\n",
    "\n",
    "2. It sorts all key-value pairs by using keys, returning a <Key, List<Value>> output with sorted key-value pairs.\n",
    "\n",
    "\n",
    "- A **Reduce** function: It takes list of <Key, List<Value>> sorted pairs from the Shuffle function and performs an aggregation of values, based on their keys. \n",
    "\n",
    "\n",
    "\n",
    "**Hadoop MapReduce**\n",
    "\n",
    "Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.\n",
    "\n",
    "A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.\n",
    "\n",
    "Typically the compute nodes and the storage nodes are the same, that is, the MapReduce framework and the Hadoop Distributed File System (see HDFS Architecture Guide) are running on the same set of nodes. This configuration allows the framework to effectively schedule tasks on the nodes where data is already present, resulting in very high aggregate bandwidth across the cluster.\n",
    "\n",
    "Source: https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTDTiT2_Nm2L"
   },
   "source": [
    "## Spark vs Hadoop\n",
    "\n",
    "- Data storage: in-memory vs disk\n",
    "- Fault tolerance: RDDs guarantee fault tolerance; \n",
    "- Hadoop uses replication to achieve fault tolerance\n",
    "- Spark is faster due to in-memory computation\n",
    "- Spark benefits of Linux, Windows and MacOS support\n",
    "- Spark can be used to process and modify real-time data; Hadoop Map-Reduce can process a batch of stored data\n",
    "\n",
    "<img src=\"https://data-flair.training/blogs/wp-content/uploads/sites/2/2016/09/Hadoop-MapReduce-vs-Apache-Spark.jpg\"></img>\n",
    "*Source: DataFlair*\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/acadgildsite/wordpress_images/bigdatadeveloper/10+steps+to+master+apache+spark/hadoop_spark_1.png\"></img>\n",
    "\n",
    "*Source: MongoDB*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJj5enyrPwax"
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "Execute the cells below to:\n",
    "- Install PySpark\n",
    "- Copy the San Francisco Bay Area Bike Share dataset inside your data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbgBh6SNNnpA"
   },
   "source": [
    "## Batch vs Stream processing examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "_Bl7WeRsJAvd",
    "outputId": "7ff674c1-c029-42dc-c1d0-7d665e8d005c"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeZQkj4nJA_U"
   },
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHdiuKMSpWSa"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "#station_rdd=sc.wholeTextFiles(\"s3://station.csv\")\n",
    "station_df = sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/station.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "kAEicVuUMAMV",
    "outputId": "c92e9e34-8610-49d6-f67e-29d08d624425"
   },
   "outputs": [],
   "source": [
    "status_df=sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/status.csv\",header=True)\n",
    "status_df.registerTempTable(\"status\")\n",
    "status_df.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciL1QRe_Q-3k"
   },
   "source": [
    "**Measuring popularity of each station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "75JHxA3IOfgE",
    "outputId": "5a29f82b-05b5-483a-882e-8462d1889bbe"
   },
   "outputs": [],
   "source": [
    "station_id=status_df.select('station_id')\n",
    "mapped_test = station_id.rdd.map(lambda item: (item,1))\n",
    "aggregated_station_counts = mapped_test.reduceByKey(lambda a,b:a+b)\n",
    "mapped_test.take(5)\n",
    "aggregated_station_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lA4E9kpynVjc"
   },
   "outputs": [],
   "source": [
    "def processing_function(filename,caching=True):\n",
    "    rdd_sample=sc.wholeTextFiles(filename)\n",
    "    rdd_map=rdd_sample.map(lambda item: (item, 1))\n",
    "    rdd_reduce=rdd_map.reduceByKey(lambda a,b: a+b)\n",
    "    if caching:\n",
    "        rdd_reduce.persist(StorageLevel.MEMORY_AND_DISK_SER) # since we will read more than once, caching in Memory will make things quicker.\n",
    "    else:\n",
    "        rdd_reduce.persist(StorageLevel.DISK_ONLY) # Here is another option to control the caching behaviour\n",
    "    return rdd_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De-Be1L7ZJc6"
   },
   "source": [
    "## Batch vs Stream processing time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "qkVJLiRlpYlC",
    "outputId": "3e00b5a1-690e-4d0f-f56b-a7e336e6cf39"
   },
   "outputs": [],
   "source": [
    "#Running an experiment to measure time spent processing\n",
    "from pyspark import StorageLevel\n",
    "from time import time\n",
    "\n",
    "resCaching = [] # for storing results\n",
    "resNoCache = []\n",
    "\n",
    "for i in range(3): # 3 samples \n",
    "    startTime = time() # start timer\n",
    "    processing_function(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/trip.csv\",caching=True).collect()\n",
    "    endTime = time()  \n",
    "    resCaching.append( endTime - startTime )\n",
    "    \n",
    "for i in range(3): # 3 samples\n",
    "    startTime = time()\n",
    "    processing_function(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/trip.csv\",caching=False).collect()\n",
    "    endTime = time()\n",
    "    resNoCache.append( endTime - startTime )\n",
    "\n",
    "meanTimeCaching = sum(resCaching)/len(resCaching)\n",
    "meanTimeNoCache = sum(resNoCache)/len(resNoCache)\n",
    "\n",
    "print('Running item count on the station dataset, 3 trials - mean time with caching: ', meanTimeCaching, ', mean time without caching: ', meanTimeNoCache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReQIjT86ZWzf"
   },
   "source": [
    "## Spark SQL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "yd6N9BfapZ5i",
    "outputId": "ecda6d7a-40da-4f8f-da95-d6519aefa57e"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/station.csv\",header=True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "xITHD_enpaGP",
    "outputId": "63f1262e-2a17-4bfb-9e2e-76438c95b24f"
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "YWj2begOr4__",
    "outputId": "5297bdff-ef7e-45da-e042-d2e1d7e1ac2f"
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"station\")\n",
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "dw9JOx1Xr9xp",
    "outputId": "b1ac85e1-d7e7-4676-e6d6-de1906242d35"
   },
   "outputs": [],
   "source": [
    "san_jose= sqlContext.sql(\"SELECT * FROM station where city=='San Jose' LIMIT 100 \")\n",
    "san_jose.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7mSMfd6ZXK9"
   },
   "source": [
    "## Pandas vs Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHLKPFgipbDg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "start_time=time()\n",
    "pandas_df=pd.read_csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/status.csv\")\n",
    "end_time=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4d7AldxClDz"
   },
   "outputs": [],
   "source": [
    "start_time_spark=time()\n",
    "status_df = sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/status.csv\",header=True)\n",
    "end_time_spark=time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "wlMvU4NNCa8n",
    "outputId": "805da7cb-67dd-44f9-fe76-c63c4d251ccd"
   },
   "outputs": [],
   "source": [
    "print(\"Time spent to load the Pandas DataFrame in memory:\", end_time - start_time )\n",
    "print(\"Time spent to load the Spark DataFrame in memory:\", end_time_spark - start_time_spark )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "WSxStaZnpbQp",
    "outputId": "128eb2c7-7b71-4430-b780-cb0a33cabacc"
   },
   "outputs": [],
   "source": [
    "pandas_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "wvnKVLnsCDMX",
    "outputId": "180e113c-f907-44eb-c96f-35d5ae9599c3"
   },
   "outputs": [],
   "source": [
    "pandas_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Flubdu40EuG3",
    "outputId": "b5e0a63e-9605-4643-e17f-a413a11bbf09"
   },
   "outputs": [],
   "source": [
    "status_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "eFqrajikFcGX",
    "outputId": "a7279d65-5c59-4bb4-fec8-5af58e8cb927"
   },
   "outputs": [],
   "source": [
    "status_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0A3cJA83FmSJ",
    "outputId": "ab4ada4c-ec11-44f5-a915-672e4ae4fc62"
   },
   "outputs": [],
   "source": [
    "status_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azq61mbaZnNH"
   },
   "source": [
    "##  Inspect data\n",
    "Other functions that you can use to inspect your data are take() or takeSample(), but also countByKey(), countByValue() or collectAsMap()\n",
    "\n",
    "Add a few more commands from \n",
    "https://www.datacamp.com/community/tutorials/apache-spark-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "cDOhisbbsnbJ",
    "outputId": "882e7683-e57e-4fd0-cab0-a24217a3436f"
   },
   "outputs": [],
   "source": [
    "san_jose.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "l1B_U57ksnqf",
    "outputId": "c4b46def-e55f-4b84-b3f8-40c6c5db0037"
   },
   "outputs": [],
   "source": [
    "#Return a fixed-size sampled subset of this RDD (currently requires numpy).\n",
    "station_rdd.takeSample(False,20,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "E9nwZVRjsn26",
    "outputId": "b58ecada-ce01-4395-e4ab-8d7742907952"
   },
   "outputs": [],
   "source": [
    "#countByKey:  Count the number of elements for each key, and return the result to the master as a dictionary.\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1),('c',4)])\n",
    "sorted(rdd.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "efByS5N8t6JJ",
    "outputId": "3d0f2c25-1d82-4bf6-d815-1f8edc2a8f08"
   },
   "outputs": [],
   "source": [
    "#countByValue:  Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.\n",
    "sorted(sc.parallelize([1, 2, 1, 2, 2,3]).countByValue().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrMmnw9pZvDo"
   },
   "source": [
    "## Creating new columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "C4deyf2-t6Zq",
    "outputId": "85b1d5cf-d50e-4b27-e575-fcfb3d210fb4"
   },
   "outputs": [],
   "source": [
    "status_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRXBbi_2TA3a"
   },
   "source": [
    "The San Francisco district is cutting budgets for the Bike Sharing programme. It has decided to take out one bicycle at every station. Let's see how many bicycles are available tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "Y-LPPGL6pb8m",
    "outputId": "36f0d245-6a52-45cd-94b5-b91755932c64"
   },
   "outputs": [],
   "source": [
    "status_df.withColumn('bikes_available_tomorrow', status_df.bikes_available -1 ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jOMTFfFTmqG"
   },
   "source": [
    "Documentation available at: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWQcdPWqdZuO"
   },
   "source": [
    "## Exercise 1\n",
    "<p>Create a Spark RDD based on the status.csv file. Follow the examples above and extract 10 rows from the dataset</p>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VH_VqGb4c3rv"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRRFWjMsc_-B"
   },
   "outputs": [],
   "source": [
    "status_df = sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/status.csv\",header=True)\n",
    "status_rdd=status_df.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohxV79ovdAaT"
   },
   "outputs": [],
   "source": [
    "status_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDFHVCBuex_4"
   },
   "source": [
    "## Exercise 2\n",
    "<p>Practice the MapReduce algorithm and measure the popularity of each station by counting the number of station occurences.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKU63ZtpdAmN"
   },
   "outputs": [],
   "source": [
    "station_df = sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/station.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHUIzSckdAxM"
   },
   "outputs": [],
   "source": [
    "station_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaucn8QcdA8Q"
   },
   "outputs": [],
   "source": [
    "station_rdd=station_df.rdd\n",
    "station_rdd_mapped=station_rdd.map(lambda x: (x[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVU09wIYs150"
   },
   "outputs": [],
   "source": [
    "station_rdd_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9fzP0tms18A"
   },
   "outputs": [],
   "source": [
    "station_count=station_rdd_mapped.reduceByKey(lambda a,b:a+b)\n",
    "station_count.take(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJdtG8SJe6-5"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "Use Spark SQL to create a Spark Dataframe from the station.csv file.\n",
    "\n",
    "Select all stations from the dataframe where the dock count is lower than 20. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bofIvxGqe744"
   },
   "outputs": [],
   "source": [
    "station_df = sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/station.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgU7DbPNe8LC"
   },
   "outputs": [],
   "source": [
    "station_df.createOrReplaceTempView(\"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJ6DSmr5e8nm"
   },
   "outputs": [],
   "source": [
    "low_dock_count=sqlContext.sql(\"SELECT * FROM station where dock_count < 20\")\n",
    "low_dock_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCR6fhUHfeI6"
   },
   "source": [
    "# Exercise 4\n",
    "\n",
    "4.1 Create a new column to show the bike share trip ride, expressed in minutes. \n",
    "\n",
    "**Hint**: Use the (.withColumn example above) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gIv6az2ht_S"
   },
   "outputs": [],
   "source": [
    "trip_df= sqlContext.read.csv(\"s3://msin0166-spark-workshop/data/sf_bike_sharing_data/trip.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "OOGg7mfjhuN-",
    "outputId": "eac788b8-2db2-473d-a8e4-7e1732ffe13a"
   },
   "outputs": [],
   "source": [
    "trip_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "ynbzr2aPvDZh",
    "outputId": "17f2e95b-d285-48a5-85f0-a0a7541f070d"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "trip_df=trip_df.withColumn(\"time\",(unix_timestamp(trip_df.end_date,format=\"mm/dd/yyyy HH:mm\")- unix_timestamp(trip_df.start_date,format=\"m/dd/yyyy HH:mm\"))/60)\n",
    "trip_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAs8z663hx48"
   },
   "source": [
    "4.2 Finally, return all records in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbClBuOQvC4X"
   },
   "outputs": [],
   "source": [
    "#In order to return all records in a dataset, the collect() function is applied:\n",
    "#trip_df.rdd.collect()\n",
    "\n",
    "\n",
    "#The collect () function will process your entire dataset. It may take a long time and it may also crash due to limited compute availability.\n",
    "#As an alternative, you could Spark SQL instead. \n",
    "\n",
    "trip_df.createOrReplaceTempView(\"new_trip_table\")\n",
    "new_trip_table=sqlContext.sql(\"SELECT * FROM new_trip_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XUZPqA17HP4G",
    "outputId": "2262e1f0-2c2b-4c1a-d283-709ff134389c"
   },
   "outputs": [],
   "source": [
    "new_trip_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCtnPlXKIDZt"
   },
   "outputs": [],
   "source": [
    "new_trip_table.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4RHJHsekThS"
   },
   "source": [
    "# Exercise 5\n",
    "Count the number of occurences for each city in the station dataset by applying the MapReduce algorithm, similar to the Word Count example in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xud38rA3O7UP"
   },
   "outputs": [],
   "source": [
    "#Similar example, counting the city popularity\n",
    "station_rdd_mapped=station_rdd.map(lambda x: (x[5],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfKzAPG9uzO3"
   },
   "outputs": [],
   "source": [
    "station_rdd_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6U48CBkuzSl"
   },
   "outputs": [],
   "source": [
    "city_popularity=station_rdd_mapped.reduceByKey(lambda a,b:a+b)\n",
    "city_popularity.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0XQJAq4uzUq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iu0wl_FruzRY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PySpark_workshop_practice_Week_2_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
