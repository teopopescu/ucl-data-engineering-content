{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSIN0166 Data Engineering workshop practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will explore how to extract and process data for analytical insights via PySpark and SparkSQL.\n",
    "You will now use S3 as a data source to extract data instead of connecting to a Postgres database. \n",
    "<br/>\n",
    "<br/>\n",
    "Data from the Postgres database has been copied into S3 in the following S3 bucket: <b>s3://msin0166-spark-workshop</b>\n",
    "<br/>\n",
    "It can be found within the  <b>data/ecommerce_data workshop</b>. \n",
    "<br/>\n",
    "<br/>\n",
    "Hence, all table data can be found at: <b>s3://msin0166-spark-workshop/data/ecommerce_data/</b>\n",
    "\n",
    "Note: All tables have been stored in Parquet format. \n",
    "In order to check the list of tables stored in S3 (Parquet files), please navigate to the msin0166-spark-workshop S3 bucket via the AWS Management console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set the accessKeyID and the secretAccessKey variables in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"S3CSVRead\").getOrCreate()\n",
    "accessKeyId=\" \"\n",
    "secretAccessKey=\" \"\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", accessKeyId)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", secretAccessKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Follow the sample code below and load the Order table data in a Spark DataFrame\n",
    "\n",
    "<code>station_df = spark.read.parquet(\"s3://msin0166-spark-workshop/data/ecommerce_data/user.parquet\")</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order_df = spark.read.parquet(\".....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3:  In order to display the Spark DataFrame, we will call the show() function on the Dataframe\n",
    "Example: <code> station_df.show() </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please display the DataFrame in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides the SparkSQL library, allowing us to write SQL against DataFrames. In order to do that, we need to store our DataFrame as a view, as follows:\n",
    "<br/>\n",
    "<br/>\n",
    "<code> station_df.createOrReplaceTempView(\"station_df\")</code>\n",
    "<br/>\n",
    "<br/>\n",
    "We can then start writing SQL queries to extract data from the DataFrame\n",
    "\n",
    "<code>query_result=spark.sql(\"SELECT * FROM station_df\")</code>\n",
    "<br/>\n",
    "<br/>\n",
    "<code>query_result.show()</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Select the average of the order total in the Order table by using SparkSQL\n",
    "Note: Don't forget to create a view, so that the order table can be found by your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the query result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: \n",
    "a) Extract the user data from S3. <br/>\n",
    "b) Register it as a view <br/>\n",
    "c) Count how many users are less than 60 years old <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Extract the user data from S3.\n",
    "user_data=spark.read.parquet(\".....\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Register it as a view\n",
    "user_data....(\"user_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Count how many users are less than 60 years old\n",
    "user_count=spark.sql(\"....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Retrieve user data together with all personal identifiable information. (Hint: join PIIand user tables)\n",
    "a) Extract PII data from S3 (Hint: It's the pii.parquet file) <br/>\n",
    "b) Register the PII data as a view <br/>\n",
    "c) Write a SQL query to JOIN the PII table and the User table  <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Extract PII data from S3 (Hint: It's the pii.parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Register the PII data as a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Write a SQL query to JOIN the PII table and the User table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Find all products ordered by the user whose first name is ‘FN1’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create new Spark DataFrame columns by using the .withColumn() function <br/>\n",
    "<br/>\n",
    "<br/>\n",
    "The withColumn function will take two parameters:\n",
    "- First parameter representing hte name of the new column.\n",
    "- Second parameter: The expression used to populate that column\n",
    "<br/>   \n",
    "<br/>\n",
    "<code>station_df.withColumn(\"bikeParts\",concat(col(\"bike_part_one\"), col((\"bike_part_two\")))).show()<code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create a new column named fullName on your PII table. It should contain a concatenation of the first_name and last_name columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat\n",
    "pii=pii.withColumn(\"fullName\",......)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pii.createOrReplaceTempView(\"pii\")\n",
    "pii.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Create a new column called doubleProductPrice in the product table. \n",
    "It should contain values equal to double the price fomr the product_price column <br/>\n",
    "<br/>\n",
    "a) Extract Product data from S3 (Hint: It's the product.parquet file) <br/>\n",
    "b) Register the Product data as a view <br/>\n",
    "c) Follow the example in step 7 and create a new column called doubleProductPrice by doubling the value of the product_price column <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Extract Product data from S3 (Hint: It's the product.parquet file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Register the Product data as a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Follow the example in step 7 and create a new column called doubleProductPrice by doubling the value of the product_price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Spark uses the Hadoop MapReduce framework as its foundation, it would be good to apply the MapReduce algorithm as part of our exercises\n",
    "<br/>\n",
    "<br/>\n",
    "<b>Below, you can find code used to count the number of orders by their status in the Order table.<b/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+-------------------+-------------------+\n",
      "|order_id|order_total|order_status|         created_at|         updated_at|\n",
      "+--------+-----------+------------+-------------------+-------------------+\n",
      "|       1|       60.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       2|      120.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       3|      190.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       4|       30.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       5|       30.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       6|      160.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       7|      100.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       8|       30.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|       9|       20.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|      10|      180.0|    finished|2021-02-04 00:00:00|2021-02-04 00:00:00|\n",
      "|      11|       60.0| in_progress|2021-02-04 00:00:00|2021-02-05 00:00:00|\n",
      "|      12|      120.0| in_progress|2021-02-04 00:00:00|2021-02-05 00:00:00|\n",
      "|      13|      190.0| in_progress|2021-02-04 00:00:00|2021-02-05 00:00:00|\n",
      "|      14|       30.0| in_progress|2021-02-04 00:00:00|2021-02-06 00:00:00|\n",
      "|      15|       70.0| in_progress|2021-02-04 00:00:00|2021-02-06 00:00:00|\n",
      "|      16|      160.0| in_progress|2021-02-04 00:00:00|2021-02-07 00:00:00|\n",
      "|      17|      100.0| in_progress|2021-02-04 00:00:00|2021-02-08 00:00:00|\n",
      "|      18|       30.0| in_progress|2021-02-04 00:00:00|2021-02-09 00:00:00|\n",
      "|      19|       20.0| in_progress|2021-02-04 00:00:00|2021-02-10 00:00:00|\n",
      "|      20|      180.0| in_progress|2021-02-04 00:00:00|2021-02-11 00:00:00|\n",
      "+--------+-----------+------------+-------------------+-------------------+"
     ]
    }
   ],
   "source": [
    "order=spark.read.parquet(\"s3://msin0166-spark-workshop/data/ecommerce_data/order.parquet\")\n",
    "order.createOrReplaceTempView(\"order\")\n",
    "order.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order_rdd=order.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(order_id=1, order_total=60.0, order_status='finished', created_at=datetime.datetime(2021, 2, 4, 0, 0), updated_at=datetime.datetime(2021, 2, 4, 0, 0)), Row(order_id=2, order_total=120.0, order_status='finished', created_at=datetime.datetime(2021, 2, 4, 0, 0), updated_at=datetime.datetime(2021, 2, 4, 0, 0)), Row(order_id=3, order_total=190.0, order_status='finished', created_at=datetime.datetime(2021, 2, 4, 0, 0), updated_at=datetime.datetime(2021, 2, 4, 0, 0))]"
     ]
    }
   ],
   "source": [
    "order_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order_map=order_rdd.map(lambda x: (x[2],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('finished', 1), ('finished', 1), ('finished', 1)]"
     ]
    }
   ],
   "source": [
    "order_map.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order_status_count=order_map.reduceByKey(lambda a,b:a+b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('finished', 10), ('in_progress', 10)]"
     ]
    }
   ],
   "source": [
    "order_status_count.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Use the MapReduce algorithm to count the number of products per category in the Product table \n",
    "\n",
    "Hint: Use the product_category column, which is the second column)\n",
    "<br/>\n",
    "<br/>\n",
    "a) Convert the Spark DataFrame to an RDD <br/>\n",
    "b) Print the RDD to check it is not empty <br/>\n",
    "c) Apply the map function to the Order RDD <br/>\n",
    "d) Apply the reduceByKey function to the RDD created in step c) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Convert the Spark DataFrame to an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Print the RDD to check it is not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Apply the map function to the Order RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d) Apply the reduceByKey function to the RDD created in step c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
